#!/bin/bash
echo "Reading config file"
. ./crawl.config
mkdir -p $OUTPUTDIR
echo "Starting crawl"
while read asin 
do
	echo "Downloading ASIN for reviews: $asin"

	# Begin by downloading HTML
	curl -H "$USERAGENT" "https://www.amazon.com/product-reviews/$asin/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews" |

	# Save out html file for future use and pass stdout to next tool
 	tee $OUTPUTDIR/$asin.html |

	# Scrape out celwidget elements, which contain the useful Reviews information
  	scrape -b -e 'div.review > div.celwidget' |

	# Converting to json
 	xml2json |

	# Using jq to format single level json formats for easy processing
	jq -c 'if (.html.body.div | type) == "object" then (.html.body.div) else (.html.body.div[]) end | {title: .div[0].a[1]."$t", date: .div[1].span[3]."$t", author: .div[1].span[0].a."$t", rating: .div[0].a[0].i.span."$t", body: .div[3].span."$t", asin: "'$asin'"}' |

	# Save out json file for future use and pass stdout to next tool
 	tee $OUTPUTDIR/$asin.json |

	# Convert jq json output to csv and output to file
	json2csv -p -k=title,date,author,rating,body,asin \
	> $OUTPUTDIR/$asin.csv

	# Be nice to Amazon's servers
	waitperiod=$(shuf -i $WAITSTART-$WAITEND -n 1)
	echo "Sleeping for $waitperiod seconds"
	sleep $waitperiod

	
	echo "Downloading ASIN for product info: $asin"

	# Begin by downloading HTML
	curl -H "$USERAGENT" "https://www.amazon.com/dp/$asin" |

	# Save out html file for future use and pass stdout to next tool
 	tee $OUTPUTDIR/$asin.prodinfo.html |

	# Scrape out celwidget elements, which contain the useful Reviews information
  	scrape -b -e 'div#centerCol' |

	# Converting to json
 	xml2json |

	# Using jq to format single level json formats for easy processing
	jq -c 'if (.html.body.div | type) == "object" then (.html.body.div) else (.html.body.div[]) end | {price: (.div[7].div.div.table | ( if (.tr | type) == "object" then (.tr.td[1].span[0]."$t") else (.tr[1].td[1].span[0]."$t") end)), asin: "'$asin'"}' |

	# Save out json file for future use and pass stdout to next tool
 	tee $OUTPUTDIR/$asin.prodinfo.json |

	# Convert jq json output to csv and output to file
	json2csv -p -k=price,asin \
	> $OUTPUTDIR/$asin.prodinfo.csv

	# Be nice to Amazon's servers
	waitperiod=$(shuf -i $WAITSTART-$WAITEND -n 1)
	echo "Sleeping for $waitperiod seconds"
	sleep $waitperiod
done < $ASINLIST


while read asin 
do
	echo "Downloading Walmart Review: $asin"

	# Begin by downloading HTML
	curl -H "$USERAGENT" "https://www.walmart.com/reviews/product/$asin" |

	# Save out html file for future use and pass stdout to next tool
 	tee $OUTPUTDIR/$asin.html |

	# Scrape out celwidget elements, which contain the useful Reviews information
  	scrape -b -e 'div.review[itemprop="review"]' |

	# Converting to json
 	xml2json |

	# Using jq to format single level json formats for easy processing
	jq -c 'if (.html.body.div | type) == "object" then (.html.body.div) else (.html.body.div[]) end | {title: .div.div[0].div[0].div[0].div."$t", date: .div.div[0].div[0].div[1].div."$t", author: .div.div[0].div[2].div.span."$t", rating: .div.div[0].div[1].div[0].span[0].alt, body: .div.div[1].div.div.div[0].div.div."$t", asin: "'$asin'"}' |

	# Save out json file for future use and pass stdout to next tool
 	tee $OUTPUTDIR/$asin.json |

	# Convert jq json output to csv and output to file
	json2csv -p -k=title,date,author,rating,body,asin \
	> $OUTPUTDIR/$asin.csv

	# Be nice to Amazon's servers
	waitperiod=$(shuf -i $WAITSTART-$WAITEND -n 1)
	echo "Sleeping for $waitperiod seconds"
	sleep $waitperiod
done < ./walmart_list
